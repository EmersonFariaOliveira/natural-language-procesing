{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJiFwDqtf6eE"
      },
      "source": [
        "# **Case QuantumFinance - Disciplina NLP - Classificador de chamados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "***Participantes (RM - NOME):***<br>\n",
        "350565 - Emerson Faria de Oliveira<br>\n",
        "349639 - Caio Lima Uno So<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xw6WhaNo4k3"
      },
      "source": [
        "### **Crie um classificador de chamados aplicando técnicas de PLN**\n",
        "---\n",
        "\n",
        "A **QuantumFinance** tem um canal de atendimento via chat e precisar classificar os assuntos dos atendimentos para melhorar as tratativas dos chamados dos clientes. O canal recebe textos abertos dos clientes relatando o problema e/ou dúvida e depois é direcionado para algum uma área especialista no assunto para uma melhor tratativa.​\n",
        "\n",
        "Crie um modelo classificador de assuntos aplicando técnicas de PLN, que consiga classificar através de um texto o assunto conforme disponível na base de dados [1] para treinamento e validação do modelo seu modelo.​\n",
        "\n",
        "O modelo precisar atingir um score na **métrica F1 Score superior a 75%**. Utilize o dataset [1] para treinar e testar o modelo, separe o dataset em duas amostras (75% para treinamento e 25% para teste com o randon_state igual a 42).​\n",
        "\n",
        "Fique à vontade para testar e explorar as técnicas de pré-processamento, abordagens de NLP, algoritmos e bibliotecas, mas explique e justifique as suas decisões durante o desenvolvimento.​\n",
        "\n",
        "**Composição da nota:​**\n",
        "\n",
        "**50%** - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, organização do pipeline, etc.)​\n",
        "\n",
        "**50%** - Baseado na performance (score) obtida com a amostra de teste no pipeline do modelo campeão (validar com  a Métrica F1 Score). **Separar o pipeline completo do modelo campeão conforme template.​**\n",
        "\n",
        "O trabalho poderá ser feito em grupo de 2 até 4 pessoas (mesmo grupo do Startup One).\n",
        "\n",
        "**[1] = ​https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv**\n",
        "\n",
        "**[F1 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)** com average='weighted'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DMBI8SQtps1n"
      },
      "outputs": [],
      "source": [
        "# CARREGANDO O DATA FRAME\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv', delimiter=';')\n",
        "\n",
        "# Façam o download do arquivo e utilizem localmente durante os testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s__lBzDQwrcG",
        "outputId": "e2a77c32-a043-4f90-efa4-806d0f913aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21072 entries, 0 to 21071\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   id_reclamacao         21072 non-null  int64 \n",
            " 1   data_abertura         21072 non-null  object\n",
            " 2   categoria             21072 non-null  object\n",
            " 3   descricao_reclamacao  21072 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 658.6+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKC9Vhkp0BK"
      },
      "source": [
        "Bom desenvolvimento!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlxCSMk-iAdk"
      },
      "source": [
        "###**Area de desenvolvimento e validações**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O5PedDdiZLb"
      },
      "source": [
        "Faça aqui as demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, organização do pipeline, etc.)​\n",
        "\n",
        "Fique à vontade para testar e explorar as técnicas de pré-processamento, abordagens de NLP, algoritmos e bibliotecas, mas explique e justifique as suas decisões durante o desenvolvimento.​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nze8UbKhosm9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_reclamacao</th>\n",
              "      <th>data_abertura</th>\n",
              "      <th>categoria</th>\n",
              "      <th>descricao_reclamacao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3229299</td>\n",
              "      <td>2019-05-01T12:00:00-05:00</td>\n",
              "      <td>Hipotecas / Empréstimos</td>\n",
              "      <td>Bom dia, meu nome é xxxx xxxx e agradeço se vo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3199379</td>\n",
              "      <td>2019-04-02T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>Atualizei meu cartão xxxx xxxx em xx/xx/2018 e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3233499</td>\n",
              "      <td>2019-05-06T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>O cartão Chase foi relatado em xx/xx/2019. No ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3180294</td>\n",
              "      <td>2019-03-14T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>Em xx/xx/2018, enquanto tentava reservar um ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3224980</td>\n",
              "      <td>2019-04-27T12:00:00-05:00</td>\n",
              "      <td>Serviços de conta bancária</td>\n",
              "      <td>Meu neto me dê cheque por {$ 1600,00} Eu depos...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id_reclamacao              data_abertura  \\\n",
              "0        3229299  2019-05-01T12:00:00-05:00   \n",
              "1        3199379  2019-04-02T12:00:00-05:00   \n",
              "2        3233499  2019-05-06T12:00:00-05:00   \n",
              "3        3180294  2019-03-14T12:00:00-05:00   \n",
              "4        3224980  2019-04-27T12:00:00-05:00   \n",
              "\n",
              "                             categoria  \\\n",
              "0              Hipotecas / Empréstimos   \n",
              "1  Cartão de crédito / Cartão pré-pago   \n",
              "2  Cartão de crédito / Cartão pré-pago   \n",
              "3  Cartão de crédito / Cartão pré-pago   \n",
              "4           Serviços de conta bancária   \n",
              "\n",
              "                                descricao_reclamacao  \n",
              "0  Bom dia, meu nome é xxxx xxxx e agradeço se vo...  \n",
              "1  Atualizei meu cartão xxxx xxxx em xx/xx/2018 e...  \n",
              "2  O cartão Chase foi relatado em xx/xx/2019. No ...  \n",
              "3  Em xx/xx/2018, enquanto tentava reservar um ti...  \n",
              "4  Meu neto me dê cheque por {$ 1600,00} Eu depos...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drop das colunas não utilizadas e check da distribuição das classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "categoria\n",
              "Serviços de conta bancária             5161\n",
              "Cartão de crédito / Cartão pré-pago    5006\n",
              "Roubo / Relatório de disputa           4822\n",
              "Hipotecas / Empréstimos                3850\n",
              "Outros                                 2233\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop([\"data_abertura\"],axis=1, inplace=True)\n",
        "df['categoria'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline\n",
        "### Contagem de termos simples com unigrama e sem stop-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acuracia: 0.7327\n",
            "Precisao: 0.7336\n",
            "Recall: 0.7327\n",
            "F1 Score: 0.7329\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "# divisão da amostra entre treino e teste\n",
        "df_train, df_test = train_test_split(\n",
        "      df,\n",
        "      test_size = 0.25,\n",
        "      random_state = 42\n",
        ")\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_train.descricao_reclamacao)\n",
        "text_vect_train = vect.transform(df_train.descricao_reclamacao)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "# treinamento do modelo\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.descricao_reclamacao)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "# Calculo das metricas\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"Acuracia: %.4f\" %(accuracy_score(y_test, y_pred)))\n",
        "print(\"Precisao: %.4f\" % (precision_score(y_test, y_pred, average='weighted')))\n",
        "print(\"Recall: %.4f\" % (recall_score(y_test, y_pred, average='weighted')))\n",
        "print(\"F1 Score: %.4f\" % (f1_score(y_test, y_pred, average='weighted')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Experimento 1] - Contagem de termos simples com unigrama + brigrama e sem stop-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acuracia: 0.7392\n",
            "Precisao: 0.7394\n",
            "Recall: 0.7392\n",
            "F1 Score: 0.7389\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops)\n",
        "vect.fit(df_train.descricao_reclamacao)\n",
        "text_vect_train = vect.transform(df_train.descricao_reclamacao)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "# treinamento do modelo\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.descricao_reclamacao)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"Acuracia: %.4f\" %(accuracy_score(y_test, y_pred)))\n",
        "print(\"Precisao: %.4f\" % (precision_score(y_test, y_pred, average='weighted')))\n",
        "print(\"Recall: %.4f\" % (recall_score(y_test, y_pred, average='weighted')))\n",
        "print(\"F1 Score: %.4f\" % (f1_score(y_test, y_pred, average='weighted')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Experimento 2] - TF-IDF com unigrama e com stop-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acuracia: 0.7578\n",
            "Precisao: 0.7578\n",
            "Recall: 0.7578\n",
            "F1 Score: 0.7577\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
        "vect.fit(df_train.descricao_reclamacao)\n",
        "text_vect_train = vect.transform(df_train.descricao_reclamacao)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "# treinamento do modelo\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.descricao_reclamacao)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"Acuracia: %.4f\" %(accuracy_score(y_test, y_pred)))\n",
        "print(\"Precisao: %.4f\" % (precision_score(y_test, y_pred, average='weighted')))\n",
        "print(\"Recall: %.4f\" % (recall_score(y_test, y_pred, average='weighted')))\n",
        "print(\"F1 Score: %.4f\" % (f1_score(y_test, y_pred, average='weighted')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Experimento 3] - TF-IDF com bigrama e sem stop-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acuracia: 0.7589\n",
            "Precisao: 0.7605\n",
            "Recall: 0.7589\n",
            "F1 Score: 0.7593\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=stops)\n",
        "vect.fit(df_train.descricao_reclamacao)\n",
        "text_vect_train = vect.transform(df_train.descricao_reclamacao)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "# treinamento do modelo\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.descricao_reclamacao)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"Acuracia: %.4f\" %(accuracy_score(y_test, y_pred)))\n",
        "print(\"Precisao: %.4f\" % (precision_score(y_test, y_pred, average='weighted')))\n",
        "print(\"Recall: %.4f\" % (recall_score(y_test, y_pred, average='weighted')))\n",
        "print(\"F1 Score: %.4f\" % (f1_score(y_test, y_pred, average='weighted')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Experimento 4] - TF-IDF com unigrama e sem stop-words em textos lematizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "correr 1 , 2 , 3\n"
          ]
        }
      ],
      "source": [
        "# teste das funções de lematização\n",
        "import spacy\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# função de lematização completa do documento\n",
        "def lemmatizer_text(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      sent.append(word.lemma_)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# validação das funções\n",
        "print(lemmatizer_text('correndo 1, 2, 3'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acuracia: 0.7747\n",
            "Precisao: 0.7746\n",
            "Recall: 0.7747\n",
            "F1 Score: 0.7746\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_train['text_lemma'] = df_train.descricao_reclamacao.apply(lemmatizer_text)\n",
        "\n",
        "# vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops)\n",
        "vect.fit(df_train.text_lemma)\n",
        "text_vect_train = vect.transform(df_train.text_lemma)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "# treinamento do modelo\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_test['text_lemma'] = df_test.descricao_reclamacao.apply(lemmatizer_text)\n",
        "x_test = vect.transform(df_test.text_lemma)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"Acuracia: %.4f\" %(accuracy_score(y_test, y_pred)))\n",
        "print(\"Precisao: %.4f\" % (precision_score(y_test, y_pred, average='weighted')))\n",
        "print(\"Recall: %.4f\" % (recall_score(y_test, y_pred, average='weighted')))\n",
        "print(\"F1 Score: %.4f\" % (f1_score(y_test, y_pred, average='weighted')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Experimento 5] - TF-IDF com bigrama e sem stop-words (SpaCy e NLTK) em textos lematizados e normalizados. **(Campeão)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re \n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stopwords = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "stopwords =  stopwords + ['xxxx', 'xxx', 'xx' ,'chase']\n",
        "\n",
        "def lemmatizer_verbs(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      if word.pos_ == \"VERB\":\n",
        "          sent.append(word.lemma_)\n",
        "      else:\n",
        "          sent.append(word.text)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# função que remove pontuação\n",
        "def remove_punctuation(text):\n",
        "    punctuations = string.punctuation\n",
        "    table = str.maketrans({key: \" \" for key in punctuations})\n",
        "    text = text.translate(table)\n",
        "    return text\n",
        "\n",
        "# função que normaliza o texto e remove stopwords\n",
        "def norm_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = remove_punctuation(text)\n",
        "    text = \"\".join([w for w in text if not w.isdigit()])\n",
        "    text = word_tokenize(text)\n",
        "    text = [x for x in text if x not in stopwords]\n",
        "    text = [y for y in text if len(y) > 2]\n",
        "    text = \" \".join([t for t in text])\n",
        "    return text\n",
        "\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_train['text_lemma'] = df_train.descricao_reclamacao.apply(norm_tokenize)\n",
        "df_train['text_lemma'] = df_train.descricao_reclamacao.apply(lemmatizer_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acuracia: 0.9011\n",
            "Precisao: 0.9016\n",
            "Recall: 0.9011\n",
            "F1 Score: 0.9007\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=stops)\n",
        "vect.fit(df_train.text_lemma)\n",
        "text_vect_train = vect.transform(df_train.text_lemma)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "# treinamento do modelo\n",
        "# model = DecisionTreeClassifier(random_state=42)\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_test['text_lemma'] = df_test.descricao_reclamacao.apply(norm_tokenize)\n",
        "df_test['text_lemma'] = df_test.descricao_reclamacao.apply(lemmatizer_verbs)\n",
        "\n",
        "x_test = vect.transform(df_test.text_lemma)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"Acuracia: %.4f\" %(accuracy_score(y_test, y_pred)))\n",
        "print(\"Precisao: %.4f\" % (precision_score(y_test, y_pred, average='weighted')))\n",
        "print(\"Recall: %.4f\" % (recall_score(y_test, y_pred, average='weighted')))\n",
        "print(\"F1 Score: %.4f\" % (f1_score(y_test, y_pred, average='weighted')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Experimento 6] - Classificador com word2vec (Normalização/Tokenização)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download do arquivo no repositório do professor\n",
        "!wget 'https://dados-ml-pln.s3-sa-east-1.amazonaws.com/skip_s300.zip'\n",
        "\n",
        "# Descompactação do arquivo\n",
        "!unzip 'skip_s300.zip' # subistitua com nome do arquivo\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.KeyedVectors at 0x20fb1b23a00>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load do modelo pelo Gensim\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model_skip = KeyedVectors.load_word2vec_format('skip_s300.txt')\n",
        "model_skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Normalização de texto\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# lista de stopwords do NLTK\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# função que remove pontuação\n",
        "def remove_punctuation(text):\n",
        "    punctuations = string.punctuation\n",
        "    table = str.maketrans({key: \" \" for key in punctuations})\n",
        "    text = text.translate(table)\n",
        "    return text\n",
        "\n",
        "# função que normaliza o texto e remove stopwords\n",
        "def norm_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = remove_punctuation(text)\n",
        "    text = \"\".join([w for w in text if not w.isdigit()])\n",
        "    text = word_tokenize(text)\n",
        "    text = [x for x in text if x not in stopwords]\n",
        "    text = [y for y in text if len(y) > 2]\n",
        "    #return \" \".join([t for t in text])\n",
        "    return text\n",
        "\n",
        "# Função para obter embeddings médios para cada texto\n",
        "vectorizer = model_skip\n",
        "\n",
        "def average_vector(words):\n",
        "  vectors = [vectorizer[word] for word in words if word in vectorizer]\n",
        "  if vectors:\n",
        "    return sum(vectors) / len(vectors)\n",
        "  else:\n",
        "    vector_size = vectorizer.vector_size\n",
        "    wv_res = np.zeros(vector_size)\n",
        "    return wv_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nosre\\Desktop\\GIT\\natural-language-procesing\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     precision    recall  f1-score   support\n",
            "\n",
            "Cartão de crédito / Cartão pré-pago       0.87      0.85      0.86      1290\n",
            "            Hipotecas / Empréstimos       0.84      0.87      0.85       922\n",
            "                             Outros       0.85      0.71      0.78       549\n",
            "       Roubo / Relatório de disputa       0.79      0.82      0.80      1204\n",
            "         Serviços de conta bancária       0.86      0.90      0.88      1303\n",
            "\n",
            "                           accuracy                           0.84      5268\n",
            "                          macro avg       0.84      0.83      0.83      5268\n",
            "                       weighted avg       0.84      0.84      0.84      5268\n",
            "\n",
            "0.8432042520880789\n",
            "\n",
            "Acuracia: 0.8432\n",
            "Precisao: 0.8438\n",
            "Recall: 0.8432\n",
            "F1 Score: 0.8426\n"
          ]
        }
      ],
      "source": [
        "# Tokenizando e normalizando o texto\n",
        "df_train['tokens'] = df_train.descricao_reclamacao.apply(norm_tokenize) \n",
        "# Converter textos para embeddings com Word2Vec\n",
        "df_train[\"vetor\"] = df_train[\"tokens\"].apply(average_vector)\n",
        "\n",
        "# variáveis\n",
        "x_train = df_train[\"vetor\"]\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "# Treinar Classificador\n",
        "model = LogisticRegression(random_state=42)\n",
        "# model = RandomForestClassifier(random_state=42)\n",
        "model.fit(list(x_train), y_train)\n",
        "\n",
        "# Avaliação do modelo\n",
        "# Tokenizando e normalizando o texto\n",
        "df_test['tokens'] = df_test.descricao_reclamacao.apply(norm_tokenize) \n",
        "# Converter textos para embeddings com Word2Vec\n",
        "df_test[\"vetor\"] = df_test[\"tokens\"].apply(average_vector)\n",
        "\n",
        "x_test = df_test[\"vetor\"]\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "y_pred = model.predict(list(x_test))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print()\n",
        "print(\"Acuracia: %.4f\" %(accuracy_score(y_test, y_pred)))\n",
        "print(\"Precisao: %.4f\" % (precision_score(y_test, y_pred, average='weighted')))\n",
        "print(\"Recall: %.4f\" % (recall_score(y_test, y_pred, average='weighted')))\n",
        "print(\"F1 Score: %.4f\" % (f1_score(y_test, y_pred, average='weighted')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Experimento 7] - LDA com unigrama e sem stop-words (SpaCy e NLTK) em textos lematizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nosre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THE TOP 15 WORDS FOR TOPIC # 0: \n",
            "['00', 'pagamento', 'pagar', 'crédito', 'saldo', 'cartão', 'taxa', 'chase', 'juros', 'taxas', 'pagamentos', 'cobrar', 'conta', 'ter', 'valor']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC # 1: \n",
            "['conta', 'xxxx', 'chase', 'dizer', 'ter', 'fazer', 'poder', 'dinheiro', 'xx', 'banco', 'cartão', '00', 'cheque', 'então', 'fundos']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC # 2: \n",
            "['chase', 'conta', 'crédito', 'xx', 'taxa', 'informações', 'lei', 'fornecer', 'consumidor', 'contrato', 'dever', 'ter', 'qualquer', 'taxas', 'dias']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC # 3: \n",
            "['xxxx', 'chase', 'crédito', 'ter', 'empréstimo', 'hipoteca', 'casa', 'fazer', 'relatório', 'poder', 'anos', 'pagamentos', 'dizer', 'pagar', 'nunca']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC # 4: \n",
            "['xxxx', 'xx', 'chase', '00', 'cartão', 'crédito', 'dizer', 'ter', 'enviar', 'conta', 'receber', 'recebir', 'contato', 'fazer', 'poder']\n",
            "\n",
            "\n",
            "0.38686408504176156\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stopwords = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "stopwords =  stopwords + ['xxxx']\n",
        "\n",
        "def lemmatizer_verbs(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      if word.pos_ == \"VERB\":\n",
        "          sent.append(word.lemma_)\n",
        "      else:\n",
        "          sent.append(word.text)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "df_train['text_lemma'] = df_train.descricao_reclamacao.apply(lemmatizer_verbs)\n",
        "df_test['text_lemma'] = df_test.descricao_reclamacao.apply(lemmatizer_verbs)\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_train.text_lemma)\n",
        "\n",
        "text_vect_train = vect.transform(df_train.text_lemma)\n",
        "\n",
        "# modelo\n",
        "LDA = LatentDirichletAllocation(n_components=5,random_state=42)\n",
        "LDA.fit(text_vect_train)\n",
        "\n",
        "# top palavras dos tópicos\n",
        "terms = vect.get_feature_names_out()\n",
        "\n",
        "for index, topic in enumerate(LDA.components_):\n",
        "    terms_comp = zip(terms, topic)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:15]\n",
        "    print(\"THE TOP 15 WORDS FOR TOPIC # \"+str(index)+\": \")\n",
        "    #print(sorted_terms)\n",
        "    print([t[0] for t in sorted_terms])\n",
        "    print('\\n')\n",
        "\n",
        "text_vect_test = vect.transform(df_test.text_lemma)\n",
        "results = LDA.transform(text_vect_test)\n",
        "\n",
        "df_test['topico'] = results.argmax(axis=1)\n",
        "\n",
        "df_test['categ_cod'] = df_test.categoria.map(\n",
        "    {'Serviços de conta bancária':1 , 'Cartão de crédito / Cartão pré-pago':0 , 'Roubo / Relatório de disputa':2 , 'Hipotecas / Empréstimos': 3, \"Outros\":4 }\n",
        ")\n",
        "\n",
        "print(accuracy_score(df_test.categ_cod, df_test.topico))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SiMjcWqD_m"
      },
      "source": [
        "### **Validação do professor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24EasckqG2I"
      },
      "source": [
        "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras implementações utilizadas no processo de desenvolvimento do modelo.\n",
        "\n",
        "O modelo precisar atingir um score na métrica F1 Score superior a 75%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VFA-CYfawkEJ"
      },
      "outputs": [],
      "source": [
        "# CARREGANDO O DATA FRAME\n",
        "df = pd.read_csv('https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv', delimiter=';')\n",
        "df.drop([\"data_abertura\"],axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BuJtvcfXo3J4"
      },
      "outputs": [],
      "source": [
        "# Divisão da amostra entre treino e teste\n",
        "df_train, df_test = train_test_split(\n",
        "      df,\n",
        "      test_size = 0.25,\n",
        "      random_state = 42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ULYNH6-o3Hf"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stopwords = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "stopwords =  stopwords + ['xxxx', 'xxx', 'xx' ,'chase']\n",
        "\n",
        "def lemmatizer_verbs(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      if word.pos_ == \"VERB\":\n",
        "          sent.append(word.lemma_)\n",
        "      else:\n",
        "          sent.append(word.text)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# função que remove pontuação\n",
        "def remove_punctuation(text):\n",
        "    punctuations = string.punctuation\n",
        "    table = str.maketrans({key: \" \" for key in punctuations})\n",
        "    text = text.translate(table)\n",
        "    return text\n",
        "\n",
        "# função que normaliza o texto e remove stopwords\n",
        "def norm_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = remove_punctuation(text)\n",
        "    text = \"\".join([w for w in text if not w.isdigit()])\n",
        "    text = word_tokenize(text)\n",
        "    text = [x for x in text if x not in stopwords]\n",
        "    text = [y for y in text if len(y) > 2]\n",
        "    text = \" \".join([t for t in text])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ClM-JTJo3FK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_train['text_lemma'] = df_train.descricao_reclamacao.apply(norm_tokenize)\n",
        "df_train['text_lemma'] = df_train.descricao_reclamacao.apply(lemmatizer_verbs)\n",
        "\n",
        "# Vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=stops)\n",
        "vect.fit(df_train.text_lemma)\n",
        "text_vect_train = vect.transform(df_train.text_lemma)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Teste do modelo\n",
        "df_test['text_lemma'] = df_test.descricao_reclamacao.apply(norm_tokenize)\n",
        "df_test['text_lemma'] = df_test.descricao_reclamacao.apply(lemmatizer_verbs)\n",
        "\n",
        "x_test = vect.transform(df_test.text_lemma)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Metricas\n",
        "print(\" ----------- Metricas do modelo campeao ----------- \")\n",
        "print(\"Acuracia: %.4f\" %(accuracy_score(y_test, y_pred)))\n",
        "print(\"Precisao: %.4f\" % (precision_score(y_test, y_pred, average='weighted')))\n",
        "print(\"Recall: %.4f\" % (recall_score(y_test, y_pred, average='weighted')))\n",
        "print(\"F1 Score: %.4f\" % (f1_score(y_test, y_pred, average='weighted')))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
